var store = [{
        "title": "Welcome to Jekyll!",
        "excerpt":"Welcome  ","categories": [],
        "tags": [],
        "url": "/test/",
        "teaser": null
      },{
        "title": "Temporally Reliable Motion Vectors for Better Use of Temporal Information",
        "excerpt":"Abstract   We present temporally reliable motion vectors that aim at deeper exploration of temporal coherence, especially for the generally believed difficult applications on shadows, glossy reflections, and occlusions. We show that our temporally reliable motion vectors produce significantly more robust temporal results than current traditional motion vectors while introducing negligible overhead.   Downloads   Official website   Cite   @book{Marrs2021,   title = {Ray Tracing Gems II},   editor = {Adam Marrs, Peter Shirley, and Ingo Wald},   publisher = {Apress},   year = {2021},   note ={\\url{http://raytracinggems.com/rtg2}}, }  @incollection{zeng2021temporally,   title={Temporally Reliable Motion Vectors for Better Use of Temporal Information},   author={Zeng, Zheng and Liu, Shiqiu and Yang, Jinglei and Wang, Lu and Yan, Ling-Qi},   booktitle={Ray Tracing Gems II},   pages={401--416},   year={2021},   publisher={Springer} }  Copyright Disclaimer  The electronic version of this books is free to download, and can be shared under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.  ","categories": [],
        "tags": [],
        "url": "/publications/rtg221",
        "teaser": null
      },{
        "title": "Denoising Stochastic Progressive Photon Mapping Renderings Using a Multi-Residual Network",
        "excerpt":"Abstract   Stochastic progressive photon mapping (SPPM) is one of the important global illumination methods in computer graphics. It can simulate caustics and specular-diffuse-specular lighting effects efficiently. However, as a biased method, it always suffers from both bias and variance with limited iterations, and the bias and the variance bring multi-scale noises into SPPM renderings. Recent learning-based methods have shown great advantages on denoising unbiased Monte Carlo (MC) methods, but have not been leveraged for biased ones. In this paper, we present the first learning-based method specially designed for denoising-biased SPPM renderings. Firstly, to avoid conflicting denoising constraints, the radiance of final images is decomposed into two components: caustic and global. These two components are then denoised separately via a two-network framework. In each network, we employ a novel multi-residual block with two sizes of filters, which significantly improves the model’s capabilities, and makes it more suitable for multi-scale noises on both low-frequency and high-frequency areas. We also present a series of photon-related auxiliary features, to better handle noises while preserving illumination details, especially caustics. Compared with other state-of-the-art learning-based denoising methods that we apply to this problem, our method shows a higher denoising quality, which could efficiently denoise multi-scale noises while keeping sharp illuminations.   Downloads   Paper (5MB) Slides (6MB) Presentation slides video (400MB)   Videos  Presentation slides video               Cite   @article{zeng2020denoising,   title={Denoising stochastic progressive photon mapping renderings using a multi-residual network},   author={Zeng, Zheng and Wang, Lu and Wang, Bei-Bei and Kang, Chun-Meng and Xu, Yan-Ning},   journal={Journal of Computer Science and Technology},   volume={35},   pages={506--521},   year={2020},   publisher={Springer} }  Copyright Disclaimer  © The Author(s). This is the author’s version of the work. It is posted here for your personal use. Not forredistribution. The definitive Version of Record is available at DOI.  ","categories": [],
        "tags": [],
        "url": "/publications/sppmdenoiser20",
        "teaser": null
      },{
        "title": "Joint SVBRDF Recovery and Synthesis From a Single Image using an Unsupervised Generative Adversarial Network",
        "excerpt":"Abstract   We want to recreate spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image. Pro-ducing these SVBRDFs from single images will allow designers to incorporate many new materials in their virtual scenes,increasing their realism. A single image contains incomplete information about the SVBRDF, making reconstruction difficult.Existing algorithms can produce high-quality SVBRDFs with single or few input photographs using supervised deep learning.The learning step relies on a huge dataset with both input photographs and the ground truth SVBRDF maps. This is a weaknessas ground truth maps are not easy to acquire. For practical use, it is also important to produce large SVBRDF maps. Existingalgorithms rely on a separate texture synthesis step to generate these large maps, which leads to the loss of consistency be-tween generated SVBRDF maps. In this paper, we address both issues simultaneously. We present an unsupervised generativeadversarial neural network that addresses both SVBRDF capture from a single image and synthesis at the same time. From alow-resolution input image, we generate a large resolution SVBRDF, much larger than the input images. We train a generativeadversarial network (GAN) to get SVBRDF maps, which have both a large spatial extent and detailed texels. We employ atwo-stream generator that divides the training of maps into two groups (normal and roughness as one, diffuse and specularas the other) to better optimize those four maps. In the end, our method is able to generate high-quality large scale SVBRDFmaps from a single input photograph with repetitive structures and provides higher quality rendering results with more detailscompared to the previous works. Each input for our method requires individual training, which costs about 3 hours.   Downloads   Paper (35MB) Code   Cite   @inproceedings{zhao2020joint,   title={Joint SVBRDF Recovery and Synthesis From a Single Image using an Unsupervised Generative Adversarial Network.},   author={Zhao, Yezi and Wang, Beibei and Xu, Yanning and Zeng, Zheng and Wang, Lu and Holzschuch, Nicolas},   booktitle={EGSR (DL)},   pages={53--66},   year={2020} }  Copyright Disclaimer  © The Author(s). This is the author’s version of the work. It is posted here for your personal use. Not forredistribution. The definitive Version of Record is available at DOI.  ","categories": [],
        "tags": [],
        "url": "/publications/svbrdfgan20",
        "teaser": null
      },{
        "title": "Temporally Reliable Motion Vectors for Real-time Ray Tracing",
        "excerpt":"Abstract   Real-time ray tracing (RTRT) is being pervasively applied. The key to RTRT is a reliable denoising scheme that reconstructs clean images from significantly undersampled noisy inputs, usually at 1 sample per pixel as limited by current hardware’s computing power. The state of the art reconstruction methods all rely on temporal filtering to find correspondences of current pixels in the previous frame, described using per-pixel screen-space motion vectors. While these approaches are demonstrated powerful, they suffer from a common issue that the temporal information cannot be used when the motion vectors are not valid, i.e. when temporal correspondences are not obviously available or do not exist in theory. We introduce temporally reliable motion vectors that aim at deeper exploration of temporal coherence, especially for the generally-believed difficult applications on shadows, glossy reflections and occlusions, with the key idea to detect and track the cause of each effect. We show that our temporally reliable motion vectors produce significantly better temporal results on a variety of dynamic scenes when compared to the state of the art methods, but with negligible performance overhead.   Downloads   Paper (37MB) Slides (PPT, 24MB) Presentation slides video (45MB) Supplemental video (629MB)   Videos  Presentation slides video               Supplemental video               Cite   @inproceedings{zeng2021temporally,   title={Temporally Reliable Motion Vectors for Real-time Ray Tracing},   author={Zeng, Zheng and Liu, Shiqiu and Yang, Jinglei and Wang, Lu and Yan, Ling-Qi},   booktitle={Computer Graphics Forum},   volume={40},   number={2},   pages={79--90},   year={2021},   organization={Wiley Online Library} }  Copyright Disclaimer  © The Author(s). This is the author’s version of the work. It is posted here for your personal use. Not forredistribution. The definitive Version of Record is available at DOI.  ","categories": [],
        "tags": [],
        "url": "/publications/trmv21",
        "teaser": null
      }]
